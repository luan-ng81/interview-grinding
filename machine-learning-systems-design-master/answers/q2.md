# Answer to question 2

**Question 2**

Given a dataset of credit card purchases information, each record is labelled as fraudulent or safe, how would you build a fraud detection algorithm?

**Answer**

## Data
1. Transactional, real-time, time series??, large vol (vs. insurance fraud:
   batch, smaller vol)
2. 2-Label: fraud or safe
   1. Definition of CC Fraud: unauthorized purchases, "unquantified" charges
3. Online vs offline (batch?)
   - offline: historical data, less proactive -> reactive
   - online: real time data, more proactive
4. Data Collection: on-prem, vendors, third party (Experian, Transunion etc.)
5. Geo-location
6. Connections: does the transaction connect the previous/ known frauds?
7. Sharing fraud information among banks, government agency
8. Social media?
9. Demographic : education, income, house sq footage, etc.
10. Prior defaults, collections
11. Court orders, DOJ cases

## Data modeling:
```
- Transactional data

| trans | time             | location | amount charged | CC limit | username | label |
|-------+------------------+----------+----------------+----------+----------+-------|
|     1 | Jan 1 8:30:59:10 | Dallas   |                |          | Peter    | fraud |

- User data

| username | income | cc score |
|----------+--------+----------|
```

## Business questions
1. Threshold > $x to start sending referrals?
2. TODO Classification model: does it take into accounts of financial impact?
   top 1 score: item $500 -> charged $1000 -> loss = $500
   top 20th scores: item $10000 -> charged $100K -> loss = $90K

## Offline model
1. Training data
   - Time Range to train the model: a few weeks to a few months (e.g., 6 months)
   - Split: take into accounts transactions vs users vs time vs fraud rates vs
     location
   - Seasonality: Q1-Q4
   - Split percentage: 80%/20%, 50/50?, %70/30% depending on data and business problem
   - Positive rate: very imbalanced (model choice, metrics)
   - Data fitlers: Auditing, legal
   - Missing data

2. Model Building
   1. Recency: more recent data should be more important?
      + Effect 1: model choice
      + Effect 2: production (calibrate: longer time transaction might score
        higher)
   2. Model type: +Regression+ Classification model
      - Send alert: send scores [0-1] -> 0-1000 (>900 high; 700-899 med; low
        otherwise)
   3. Imbalanced data: 1. oversampling, undersampling, SMOTE, 2. loss
      function, 3. synthetic data (GPT, GAN)
   4. Baseline model:
      + 1. rule based (decision tree):
        * 1.e.g. trans > $100K will automatically send
           alert
        * US Credit Card spent in Syria? China?
   5. Model: 2. Logistic Regression
   6. Tree-based, boosting models: Random forest, XGBoost
   7. Network-based model: Graph Neural Network (semi-supervised learning)
      + Usually, supervised learning Feature X1 (dim: nums of trans x nums
        features, eg. 100 mils x 1000); label Y1 (100K x 1) -> doesn't work in fraud
        * XGB (X_100K*1000, Y_100K*1) -> throw away >99% data
      + *Semi-supervised GNN*: Features [X1, Y1], [X2, Null] (X2 doesn't have labels)
        * Node prediction: node type = trans
        * X1 and X2 are "connected" via shared users, location, banks, etc.

          Fraud's assumption: Frauds by "association"
          Label propagation: X1 ~ X2 -> f(X1) ~ f(X2) [loss function is optimized
          this way]
          A "closed in connections" with B (family member <- unknown)
          A fraud -> B higher risk?

          strength of connections:

          A "closed in connections" with C (business partner <- unknown)
          A fraud -> C?
   8. Feature Selection: e.g. XGBoost -> feature importance (Gini, Gain, mutual
      information, etc.)

3. Validation Metrics
   - Imbalanced data: +Accuracy+
   - Alternatives: Precision vs Recall/Sensitivity
     + business: value precision more -> less capacity to review higher vols of
       alerts
   - ROC to check model perf -> not recommended due to highly imbalanced data
   - f1 weighted PR and recall; au PR and Recall:
   - Define model score cutoff: what is the cutoff to send alert? Find sweet
     spot (2D plot) -> in practice, this takes the longest time (talk business,
     add filters)

```
     | n. of top scores | top scores | PR% | Recall% |           |
     |------------------+------------+-----+---------+-----------|
     |               50 |         1% |  89 |      10 |           |
     |              255 |         2% |  80 |      25 | <- cutoff |
     |------------------+------------+-----+---------+-----------|
     |                  |         3% |  60 |      29 |           |
```

   - "low touch": cutoff = 2% -> score >=2% -> send alerts
     + how about alerts with scores <2%? how to triage those alerts? no actions
       or randomly send to customers and/ or investigation department?


## TODO Online model (most impactful)

## Business Validation:
- Quantify model prediction/ alerts?

## UI/UX front end
- How to send alert? IM, email, phone, mail? -> user behavior?
- user: customers, or investigators

## Production
- Online Model 1: f(X)
  + x: real time data at t=9:00:00, location=NY, etc
    - score = f(x)
- Offline model 2: g(X)

## Monitoring: performance, model, impact
- 1. production metrics vs 2. research metrics (assume: it's not far for each
  other!)
- impact: Model alerts yield higher financial impact? biases?
- Legal concerns? (e.g. use credit scores, features relating race and income)
- Data monitoring: population shift, missing rates? missing features
- Model scores: reproducible? data X1 -> f(X1) != f(X1) due to environment,
  model version, software upgrades (python version, xgb version, linux version,
  pytorch version)
